{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AWGzucuFfbBn"
   },
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/pinecone-io/examples/blob/master/learn/generation/langchain/handbook/00-langchain-intro.ipynb) [![Open nbviewer](https://raw.githubusercontent.com/pinecone-io/examples/master/assets/nbviewer-shield.svg)](https://nbviewer.org/github/pinecone-io/examples/blob/master/learn/generation/langchain/handbook/00-langchain-intro.ipynb)\n",
    "\n",
    "üìö [LangChain Handbook](https://github.com/pinecone-io/examples/tree/master/generation/langchain/handbook)\n",
    "\n",
    "# Intro to LangChain\n",
    "\n",
    "LangChain is a popular framework that allow users to quickly build apps and pipelines around **L**arge **L**anguage **M**odels. It can be used to for chatbots, **G**enerative **Q**uestion-**A**nwering (GQA), summarization, and much more.\n",
    "\n",
    "<span style=\"color: #705a8e;\">\n",
    "    The core idea of the library is that we can <b>_chain_</b> together different components to create more advanced use-cases around LLMs. Chains may consist of multiple components from several modules:\n",
    "</span>\n",
    "\n",
    "* **Prompt templates**: These are predefined structures or formats that guide the generation of prompts for different applications. They can be designed to suit various use-cases such as chatbots, question-answering systems etc. For example, a template for an ELI5 (Explain Like I'm Five) question-answering system might include a simple, easy-to-understand question followed by a more detailed explanation.\n",
    "\n",
    "* **LLMs**: LLMs are AI models trained on a large corpus of text data. They have the ability to generate human-like text based on the input or prompt they are given. Examples of LLMs include models like GPT-3 and BLOOM. These models are capable of understanding and generating text for a wide range of tasks, from writing essays to answering questions and even coding.\n",
    "\n",
    "* **Agents**: In the context of LangChain, agents are entities that use LLMs to decide which actions to take based on the input they receive. Agents can utilize various tools, such as web search engines or calculators, to perform tasks. The tasks and their sequence are defined in a logical loop of operations, which can include steps like receiving input, processing it, choosing an action, and providing an output.\n",
    "\n",
    "* **Memory**: In LangChain, memory refers to the system's ability to **store and recall information**. There are two types of memory: short-term and long-term. Short-term memory refers to information that is kept available for <span style=\"color: skyblue;\"><b>immediate use</b></span>, for instance, the context of a conversation in a chatbot. Long-term memory, on the other hand, is information that the system retains over time, like <span style=\"color: skyblue;\"><b>facts or knowledge</b></span> that the system has learned or been trained on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "r-ryCeG_f_GC"
   },
   "outputs": [],
   "source": [
    "!pip install -qU langchain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mNaXrEPOhbuL"
   },
   "source": [
    "# Using LLMs in LangChain\n",
    "\n",
    "üîó LangChain supports several LLM providers, like Hugging Face and OpenAI.\n",
    "\n",
    "üîç Let's start our exploration of LangChain by learning how to use a few of these different LLM integrations.\n",
    "\n",
    "## ü§ó Hugging Face\n",
    "\n",
    "We first need to install additional prerequisite libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LWA15ZkVjg80",
    "outputId": "b38a4c6a-9d98-44b4-eb71-6e96398c647a"
   },
   "outputs": [],
   "source": [
    "!pip install -qU huggingface_hub"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m-whfR5Tjf1O"
   },
   "source": [
    "For Hugging Face models we need a Hugging Face Hub API token. We can find this by first getting an account at [HuggingFace.co](https://huggingface.co/) and clicking on our profile in the top-right corner > click *Settings* > click *Access Tokens* > click *New Token* > set *Role* to *write* > *Generate* > copy and paste the token below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sRGTytxCjKaW"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ['HUGGINGFACEHUB_API_TOKEN'] = 'HF_API_KEY'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "exAl3iQgnAra"
   },
   "source": [
    "We can then generate text using a HF Hub model (we'll use `google/flan-t5-x1`) using the Inference API built into Hugging Face Hub.\n",
    "\n",
    "_(The default Inference API doesn't use specialized hardware and so can be slow and cannot run larger models like `bigscience/bloom-560m` or `google/flan-t5-xxl`)_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üìù Notes: If using Jupyter Notebook, you might an error on import which you can resolve using `pip install pydantic==1.8`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "l7yubiSJhIfs",
    "outputId": "39f9bb8b-c116-46a3-e9be-c3b3549789a9"
   },
   "outputs": [],
   "source": [
    "from langchain import PromptTemplate, HuggingFaceHub, LLMChain\n",
    "\n",
    "# initialize HF LLM\n",
    "flan_t5 = HuggingFaceHub(\n",
    "    repo_id=\"google/flan-t5-base\", # repo_id=\"google/flan-t5-xl\" XL is not availble for free tier users\n",
    "    model_kwargs={\"temperature\":1e-10}\n",
    ")\n",
    "\n",
    "# build prompt template for simple question-answering\n",
    "template = \"\"\"Question: {question}\n",
    "\n",
    "Answer: \"\"\"\n",
    "prompt = PromptTemplate(template=template, input_variables=[\"question\"])\n",
    "\n",
    "llm_chain = LLMChain(\n",
    "    prompt=prompt,\n",
    "    llm=flan_t5\n",
    ")\n",
    "\n",
    "question = \"Which NFL team won the Super Bowl in the 2010 season?\"\n",
    "\n",
    "# print(llm_chain.run(question))\n",
    "print(llm_chain.invoke(question))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DXroZSjCKxa2"
   },
   "source": [
    "If we'd like to ask multiple questions we can by passing a list of dictionary objects, where the dictionaries must contain the input variable set in our prompt template (`\"question\"`) that is mapped to the question we'd like to ask."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4jNZgxSIJsXj",
    "outputId": "f5711d89-de5a-48d0-e815-9f186a84e807"
   },
   "outputs": [],
   "source": [
    "qs = [\n",
    "    {'question': \"Which NFL team won the Super Bowl in the 2010 season?\"},\n",
    "    {'question': \"If I am 6 ft 4 inches, how tall am I in centimeters?\"},\n",
    "    {'question': \"Who was the 12th person on the moon?\"},\n",
    "    {'question': \"How many eyes does a blade of grass have?\"}\n",
    "]\n",
    "res = llm_chain.generate(qs)\n",
    "res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7zoxlXHYLQix"
   },
   "source": [
    "It is a LLM, so we can try feeding in all questions at once:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b96WIvouLQ-7",
    "outputId": "c9ff1c1f-2991-4832-d57c-b46cc346ca64"
   },
   "outputs": [],
   "source": [
    "multi_template = \"\"\"Answer the following questions one at a time.\n",
    "\n",
    "Questions:\n",
    "{questions}\n",
    "\n",
    "Answers:\n",
    "\"\"\"\n",
    "long_prompt = PromptTemplate(\n",
    "    template=multi_template,\n",
    "    input_variables=[\"questions\"]\n",
    ")\n",
    "\n",
    "llm_chain = LLMChain(\n",
    "    prompt=long_prompt,\n",
    "    llm=flan_t5\n",
    ")\n",
    "\n",
    "qs_str = (\n",
    "    \"Which NFL team won the Super Bowl in the 2010 season?\\n\" +\n",
    "    \"If I am 6 ft 4 inches, how tall am I in centimeters?\\n\" +\n",
    "    \"Who was the 12th person on the moon?\" +\n",
    "    \"How many eyes does a blade of grass have?\"\n",
    ")\n",
    "\n",
    "print(llm_chain.invoke(qs_str))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y99CMKSbOqBy"
   },
   "source": [
    "But with this model it doesn't work too well, we'll see this approach works better with different models soon."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YpdXG9YtzrLJ"
   },
   "source": [
    "## OpenAI\n",
    "\n",
    "Start by installing additional prerequisites:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FHo2YRHPDgHH",
    "outputId": "c8fd417b-d6b4-4f8e-a8a0-a95e3e1e676f"
   },
   "outputs": [],
   "source": [
    "!pip install -qU openai"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0fOo9qQvDgkz"
   },
   "source": [
    "We can also use OpenAI's generative models. The process is similar, we need to\n",
    "give our API key which can be retrieved by signing up for an account on the\n",
    "[OpenAI website](https://openai.com/api/) (see top-right of page). We then pass the API key below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "deWmOJecfbBr"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ['OPENAI_API_KEY'] = 'OPENAI_API_KEY'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CU4xirWX-Ds4"
   },
   "source": [
    "If using OpenAI via Azure you should also set:\n",
    "\n",
    "```python\n",
    "os.environ['OPENAI_API_TYPE'] = 'azure'\n",
    "# API version to use (Azure has several)\n",
    "os.environ['OPENAI_API_VERSION'] = '2022-12-01'\n",
    "# base URL for your Azure OpenAI resource\n",
    "os.environ['OPENAI_API_BASE'] = 'https://your-resource-name.openai.azure.com'\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2AWnaTCP0Ryg"
   },
   "source": [
    "Then we decide on which model we'd like to use, there are several options but we will go with `gpt-3.5-turbo-instruct`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZhQSDoYe0ly4"
   },
   "outputs": [],
   "source": [
    "from langchain import OpenAI\n",
    "\n",
    "davinci = OpenAI(model_name='gpt-3.5-turbo-instruct')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_NvK4o6SDrs0"
   },
   "source": [
    "Alternatively if using Azure OpenAI we do:\n",
    "\n",
    "```python\n",
    "from langchain.llms import AzureOpenAI\n",
    "\n",
    "llm = AzureOpenAI(\n",
    "    deployment_name=\"your-azure-deployment\", \n",
    "    model_name=\"text-davinci-003\"\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SGL2zs3uEVj6"
   },
   "source": [
    "We'll use the same simple question-answer prompt template as before with the Hugging Face example. The only change is that we now pass our OpenAI LLM `davinci`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gVSsC3iGEPAp",
    "outputId": "1d562f8d-2fbf-4cc4-84cd-cce998720eb3"
   },
   "outputs": [],
   "source": [
    "llm_chain = LLMChain(\n",
    "    prompt=prompt,\n",
    "    llm=davinci\n",
    ")\n",
    "\n",
    "print(llm_chain.run(question))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DL-buasOKpKs"
   },
   "source": [
    "The same works again for multiple questions using `generate`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vMua1MWcKtSx",
    "outputId": "55efeae1-8c30-4069-a2a8-fb78212f8523"
   },
   "outputs": [],
   "source": [
    "qs = [\n",
    "    {'question': \"Which NFL team won the Super Bowl in the 2010 season?\"},\n",
    "    {'question': \"If I am 6 ft 4 inches, how tall am I in centimeters?\"},\n",
    "    {'question': \"Who was the 12th person on the moon?\"},\n",
    "    {'question': \"How many eyes does a blade of grass have?\"}\n",
    "]\n",
    "llm_chain.generate(qs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the below format doesn't feed the questions in iteratively but instead all in one chunk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "w2-es7SgFddS",
    "outputId": "d16b7afc-f0d1-4f6a-9d89-f6811839bb02"
   },
   "outputs": [],
   "source": [
    "qs = [\n",
    "    \"Which NFL team won the Super Bowl in the 2010 season?\",\n",
    "    \"If I am 6 ft 4 inches, how tall am I in centimeters?\",\n",
    "    \"Who was the 12th person on the moon?\",\n",
    "    \"How many eyes does a blade of grass have?\"\n",
    "]\n",
    "print(llm_chain.run(qs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can try to answer all question in one go, as mentioned, more powerful LLMs like `text-davinci-003` will be more likely to handle these more complex queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xbjxnnVzA47s",
    "outputId": "cf5397ca-9e06-4221-eb45-17b1346f6f06"
   },
   "outputs": [],
   "source": [
    "multi_template = \"\"\"Answer the following questions one at a time.\n",
    "\n",
    "Questions:\n",
    "{questions}\n",
    "\n",
    "Answers:\n",
    "\"\"\"\n",
    "long_prompt = PromptTemplate(\n",
    "    template=multi_template,\n",
    "    input_variables=[\"questions\"]\n",
    ")\n",
    "\n",
    "llm_chain = LLMChain(\n",
    "    prompt=long_prompt,\n",
    "    llm=davinci\n",
    ")\n",
    "\n",
    "qs_str = (\n",
    "    \"Which NFL team won the Super Bowl in the 2010 season?\\n\" +\n",
    "    \"If I am 6 ft 4 inches, how tall am I in centimeters?\\n\" +\n",
    "    \"Who was the 12th person on the moon?\" +\n",
    "    \"How many eyes does a blade of grass have?\"\n",
    ")\n",
    "\n",
    "print(llm_chain.run(qs_str))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ybMkI18xfbBr"
   },
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "vscode": {
   "interpreter": {
    "hash": "b8e7999f96e1b425e2d542f21b571f5a4be3e97158b0b46ea1b2500df63956ce"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
